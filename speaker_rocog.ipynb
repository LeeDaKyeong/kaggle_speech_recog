{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# speaker recog train data 생성(svm)\n",
    "\n",
    "# 정규화\n",
    "def audio_regul(y):\n",
    "    _y = librosa.util.normalize(y)\n",
    "    return _y\n",
    "\n",
    "def call_audio_librosa(path, sr = 44100):\n",
    "    y, sr = librosa.load(path,sr = sr)\n",
    "    y = audio_regul(y)\n",
    "    #y = audio_extract(y)\n",
    "    return (y, sr)\n",
    "\n",
    "def MFCC_extract(y, sr = 44100, y_len = 50000):\n",
    "    if len(y) < y_len:\n",
    "        y = np.append(y,np.zeros(y_len-len(y)))\n",
    "    else:\n",
    "        y = y[:y_len]\n",
    "\n",
    "    S = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)\n",
    "    log_S = librosa.power_to_db(S, ref=np.max)\n",
    "    return log_S\n",
    "\n",
    "def MFCC_extract_reshape(y, sr = 44100, y_len = 50000):\n",
    "    if len(y) < y_len:\n",
    "        y = np.append(y,np.zeros(y_len-len(y)))\n",
    "    else:\n",
    "        y = y[:y_len]\n",
    "    log_S = MFCC_extract(y, sr)\n",
    "    log_S_reshape = np.reshape(log_S,(log_S.shape[0]*log_S.shape[1]))\n",
    "    return log_S_reshape\n",
    "\n",
    "def train_data(path, Y, index):\n",
    "    print(\"start\")\n",
    "    li = list()\n",
    "\n",
    "    for index1 in os.listdir(path):\n",
    "        path2 = os.path.join(path, index1)  # /Users/apple/Desktop/audio_name/DK/DK_11.30\n",
    "        for index2 in os.listdir(path2):\n",
    "            path3 = os.path.join(path2, index2)  # /Users/apple/Desktop/audio_name/DK/DK_11.30/DK_sentence1\n",
    "            for index3 in os.listdir(path3):\n",
    "                path4 = os.path.join(path3,index3)  # /Users/apple/Desktop/audio_name/DK/DK_11.30/DK_sentence1/sentence1_1.wav\n",
    "\n",
    "                y, sr = call_audio_librosa(path4)\n",
    "                # y = denoising.denoise(y)\n",
    "                li.append(MFCC_extract_reshape(y, y_len=100000))\n",
    "                Y.append(index)\n",
    "    _li = np.array(li)\n",
    "\n",
    "    return (_li, Y)\n",
    "\n",
    "\n",
    "# svm model load\n",
    "def speaker_recog_model_load(path = './model/speaker_recog_svm.sav'):\n",
    "    model = pickle.load(open(path, 'rb'))\n",
    "    return model\n",
    "\n",
    "# test\n",
    "def speaker_recog(audio_path):\n",
    "    speaker = {0: \"다경\",\n",
    "               1: \"혜진\",\n",
    "               2: \"강열\",\n",
    "               3: \"이삭\",\n",
    "               4: \"태권\"}\n",
    "\n",
    "    y,sr = call_audio_librosa(audio_path)\n",
    "    mfcc = MFCC_extract_reshape(y, y_len = 100000)\n",
    "    _mfcc = np.reshape(mfcc, (1, len(mfcc)))\n",
    "    model = speaker_recog_model_load()\n",
    "    result = model.predict(_mfcc)\n",
    "    #print(speaker[int(result)])\n",
    "    return speaker[int(result)]\n",
    "\n",
    "\n",
    "def speaker_clust(path):\n",
    "\n",
    "    li = os.listdir(path)\n",
    "    li = sorted(li, key = lambda x: (int(re.sub('[^0-9]','',x)),x))\n",
    "\n",
    "    X = np.zeros((1,12544))\n",
    "    for i in li:\n",
    "        y, sr = call_audio_librosa(os.path.join(path,i))\n",
    "        mfcc = MFCC_extract_reshape(y, y_len=100000)\n",
    "        _mfcc = np.reshape(mfcc, (1, len(mfcc)))\n",
    "        X = np.vstack((X,_mfcc))\n",
    "\n",
    "    X = X[1:]\n",
    "\n",
    "    kmeans = KMeans(n_clusters=5, random_state=0).fit(X)\n",
    "    print(kmeans.labels_)\n",
    "    return kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    Y = list()\n",
    "\n",
    "    DK_path = \"./audio_name/DK\"\n",
    "    DK, Y = train_data(DK_path, Y, 0)\n",
    "    # DK_len = len(DK)\n",
    "\n",
    "    HJ_path = \"./audio_name/HJ\"\n",
    "    HJ, Y = train_data(HJ_path, Y, 1)\n",
    "    # HJ_len = len(HJ)\n",
    "\n",
    "    KY_path = \"./audio_name/KY\"\n",
    "    KY, Y = train_data(KY_path, Y, 2)\n",
    "    # KY_len = len(KY)\n",
    "\n",
    "    LS_path = \"./audio_name/LS\"\n",
    "    LS, Y = train_data(LS_path, Y, 3)\n",
    "    # LS_len = len(LS)\n",
    "\n",
    "    TK_path = \"./audio_name/TK\"\n",
    "    TK, Y = train_data(TK_path, Y, 4)\n",
    "    # TK_len = len(TK)\n",
    "\n",
    "    X = np.vstack((DK, HJ, KY, LS, TK))\n",
    "\n",
    "    # Y = np.zeros(DK_len+HJ_len+KY_len+LS_len+TK_len)\n",
    "    # Y[:DK_len] = 0\n",
    "    # Y[DK_len:HJ_len] = 1\n",
    "    # Y[DK_len+HJ_len:DK_len+HJ_len+KY_len] = 2\n",
    "    # Y[DK_len+HJ_len+KY_len:DK_len+HJ_len+KY_len+LS_len] = 3\n",
    "    # Y[DK_len+HJ_len+KY_len+LS_len:] = 4\n",
    "\n",
    "    lin_clf = svm.LinearSVC()\n",
    "    lin_clf.fit(X, Y)\n",
    "\n",
    "    filename = './model/speaker_recog_svm.sav'\n",
    "    pickle.dump(lin_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
